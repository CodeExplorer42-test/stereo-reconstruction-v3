# Project Context

## Why we are here

We are taking the synthetic stereo pair generated by **`drr_generator.py`** (`data/left_view.png`, `data/right_view.png`) and turning it into a full 3‑D volume. The whole pipeline—the one drawn in the Graphviz plan—must execute inside one evolving script: **`segment.py`**. We will grow that script until it can:

1. load the rectified left/right DRRs,
2. detect sharp boundaries,
3. seed and grow intensity‑constrained blobs,
4. match those boundaries **and** blobs across the epipolar pair,
5. feed the matches into every disparity algorithm we listed (dense, sparse, edge, region),
6. triangulate depth, form a point cloud, surface it, and save a volume.

The goal is not to prove one algorithm “best” but to let us flip between them quickly and inspect outputs.

## What must already be in place before any matching happens

* **Virtual environment** Activate the `.venv` and ensure every run is `uv`‑clean.
* **Rectification** `rectify_images.py` must have produced `data/left_rectified.png` and `data/right_rectified.png`. Horizontal scan‑lines must align; check `data/rectification_check.png`.
* **Camera parameters** `data/camera_params.md` contains K, R, t, E and F. Keep them frozen so every downstream calculation stays deterministic.
* **Boundary masks** `segment.py` already thresholds, smooths, grabs Sobel edges, and records boundary pixels as white on black (`left_boundary_image`, `right_boundary_image`).
* **Seed queues** For each boundary pixel we already step ten pixels inward along ‑∇I and enqueue the seed. This gives us a dense list of candidate interiors.
* **Blob grower** The existing eight‑connected flood‑fill, intensity tollerance `delta_I`, and minimum area filter work. They label blobs in both views.

If any of those pieces break, fix them before touching stereo matching.

## Next stretch of work inside `segment.py` (single script, iterative edits)

1. **Boundary‑to‑Boundary stereo matching**

   * Use the epipolar lookup we already computed.
   * Start with simple Chamfer or SSD along the scan‑line; later drop in SGM, block‑match, etc.

2. **Blob‑to‑Blob matching**

   * For each labelled blob on the left, search along its epipolar line for candidate blobs on the right.
   * Compare centroids, Hu moments, or region histograms.

3. **Disparity aggregation switchboard**

   * Route the chosen match sets through the method‑selection diamond (dense / sparse / edge / region).
   * Output one unified `disparity_output` array regardless of method so the reconstruction code that follows can stay identical.

4. **Depth, point cloud, surface**

   * Apply the cone‑beam depth formula using the known K and the matched disparities.
   * Emit a `.ply` or `.obj` mesh plus a simple PNG depth map for quick eyeballing.

Throughout, respect every rule in **`CLAUDE.md`**: no extra files, hard‑code real paths, keep the script reading like a laboratory notebook with numbered steps and plain‑English comments. When you add functionality, extend the existing numbered blocks instead of appending throw‑away helpers.

## How to run and evaluate

```bash
# always from repo root
source .venv/bin/activate
uv run python segment.py
```

The script prints a benchmark for each numbered stage and saves intermediate images:

* `watershed_segmentation.png` visual sanity check for boundary + blob generation
* `boundary_connectivity_analysis.png`, `gradient_analysis.png` extra debugging frames you can add ad‑hoc
* later you will append `disparity_<method>.png`, `depth_map.png`, `point_cloud.ply`, etc.

Repeat the run after each algorithm switch to compare outputs side‑by‑side.

---

Update complete—nothing else changes, and no new filenames appear.
